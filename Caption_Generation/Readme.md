# GRU and LSTM-Based Image Captioning Models

These notebooks implement an image caption generation pipeline using deep learning architectures, including GRU and LSTM. The steps covered in the notebooks include data preprocessing, feature extraction, model architecture definition, training, caption generation, and evaluation.

## **1. Import Libraries**
- Essential libraries are imported for handling data, building models, and visualizations:
  - TensorFlow/Keras for model building.
  - NumPy for numerical computations.
  - Matplotlib for plotting images.
  - `nltk` for BLEU score calculations.
  - `sklearn` for model utilities and evaluation.

---

## **2. Dataset Loading and Preprocessing**
- **Dataset**: Flickr8k, containing images and associated captions.
- **Preprocessing Steps**:
  1. Images are resized and normalized.
  2. Captions are tokenized, cleaned (special characters removed), and wrapped with start (`<start>`) and end (`<end>`) tags.
  3. A tokenizer is fit on the text data, and sequences are padded for consistent input lengths.

---

## **3. Image Feature Extraction**
- **Models Used**:
  - Pretrained **InceptionV3** or **DenseNet121** are used to extract feature embeddings for each image.
  - Features are saved and reused for training.
- **Implementation**:
  - Images are resized to fit the input size of the chosen CNN.
  - Pretrained models extract the last layer's feature vectors, which are stored in dictionaries.

---

## **4. Model Architectures**

### **LSTM-Based Model**
- Encoder:
  - Input image features are normalized and passed through a dense layer.
  - The output is repeated for each timestep.
- Decoder:
  - Captions are tokenized and embedded.
  - An LSTM processes the sequential data.
- Attention Mechanism:
  - Attention is applied to combine image and textual features.
- Fully Connected Layer:
  - Outputs a softmax distribution over the vocabulary for the next word.

### **GRU-Based Model**
- The GRU model follows a similar architecture but replaces the LSTM layers with GRU layers.
- GRUs are computationally faster and have fewer parameters than LSTMs.

---

## **5. Model Training**
- **Loss Function**: Categorical cross-entropy.
- **Optimizer**: Adam with gradient clipping for stability.
- Training is performed for multiple epochs with batch size 32.
- Generators are used to yield image features and captions in batches during training.

---

## **6. Caption Generation**
- For a given image:
  - The model predicts the next word in the sequence until the `<end>` token or maximum length is reached.
  - Captions are generated by sequentially predicting words and appending them.

---

## **7. Evaluation**
- **BLEU Score**:
  - Compares generated captions with ground truth captions using n-gram similarity.
  - BLEU-1 to BLEU-4 are calculated.
- **Semantic Distance**:
  - Embeddings for captions are computed using GloVe vectors.
  - Semantic similarity is calculated as cosine similarity, and semantic distance is derived.

---

## **8. Results and Insights**
- Generated captions are evaluated qualitatively (via visual inspection) and quantitatively (via BLEU scores and semantic similarity).
- Attention weights can be visualized to understand the focus of the model during caption generation.

---

## **9. Next Steps**
- Experiment with different hyperparameters and architectures to improve accuracy.
- Integrate newer pretrained models like Vision Transformers for feature extraction.
- Test on larger datasets like Flickr30k or MS COCO for better generalization.

---
