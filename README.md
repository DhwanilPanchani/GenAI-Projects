# Generative AI Projects Repository

Welcome to the **Generative AI Projects** repository! This repository contains a collection of projects and assignments related to Generative AI, focusing on various models and techniques used in the field. The projects are based on the topics covered in the provided syllabus, which includes a wide range of topics from foundational concepts to advanced generative models.

## Table of Contents

1. [Introduction](#introduction)
2. [Project List](#project-list)
3. [Prerequisites](#prerequisites)
4. [How to Run](#how-to-run)
5. [Contributing](#contributing)
6. [License](#license)

## Introduction

This repository is designed to help you understand and implement various **Generative AI models** using Python and popular machine learning libraries such as **TensorFlow**, **PyTorch**, **Scikit-learn**, and others. The projects cover topics from basic generative modeling techniques to advanced applications like **GANs**, **Transformers**, and **Diffusion Models**.

## Project List

Here is a list of the main projects included in this repository, aligned with the course syllabus:

### 1. Generative Modeling Basics
- **Topics Covered**: Bayesian Probability, Statistical Distributions, Gradient Descent, Optimization Functions
- **Description**: Introduction to generative modeling concepts and probabilistic models for text, images, and structured data.

### 2. Image Classification with Neural Networks
- **Topics Covered**: Neural Networks, Convolutional Neural Networks (CNN), Residual Networks (ResNet)
- **Description**: Implementing image classification tasks using CNNs and ResNet architectures.

### 3. Generative Adversarial Networks (GANs)
- **Topics Covered**: Vanilla GANs, Conditional GANs (CGAN), Deep Convolutional GAN (DCGAN), CycleGAN
- **Description**: Building various types of GANs for image generation tasks.

### 4. Large Language Models (LLMs) and NLP
- **Topics Covered**: LLMs like OpenAI's ChatGPT, Prompt Engineering, Traditional ML for NLP (e.g., LSA)
- **Description**: Working with large language models for text generation and semantic analysis.

### 5. Transformers and BERT
- **Topics Covered**: Transformers, BERT, Supervised/Unsupervised Learning for Transformers
- **Description**: Implementing transformer-based models for text classification and generation tasks.

### 6. Autoencoders and Variational Autoencoders (VAEs)
- **Topics Covered**: Stacked Autoencoders, Variational Autoencoders (VAE), Vector Quantized VAE (VQVAE), Vector Quantized GAN (VQGAN)
- **Description**: Exploring autoencoder architectures for image compression and generation.

### 7. Sequential Models and Text-to-Image Generation
- **Topics Covered**: RNNs, LSTMs, Autoregressive GANs, Diffusion Models, DALL-E
- **Description**: Implementing sequential models for text-to-image generation tasks using diffusion models like DALL-E.

### 8. Structured Data Generation
- **Topics Covered**: Bayesian Belief Networks (BN), Gaussian Mixture Model (GMM), Hidden Markov Model (HMM)
- **Description**: Generating structured data using probabilistic graphical models.

## Prerequisites

Before running these projects, ensure you have the following installed:
- Python 3.x
- Libraries:
  - TensorFlow / PyTorch
  - Scikit-learn
  - Hugging Face Transformers
  - NLTK / SpaCy / Gensim (for NLP tasks)

You can install the required dependencies using the following command:

```bash
pip install -r requirements.txt
